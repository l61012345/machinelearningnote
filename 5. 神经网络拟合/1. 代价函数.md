# 神经网络的代价函数
## 回顾 
在接下来的讲义中，我们会考虑两种分类问题：第一种是二元分类，如之前的讲义所述，y的取值只能是0或者1，输出层只有一个输出单元，假设函数的输出值是一个实数；第二种是多元分类，y的取值是一个k维的向量，输出层有k个输出单元。
## 神经网络的代价函数
假设一个神经网络训练集有m个训练样本：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})}$   
$L$表示神经网络的总层数，$s_l$表示$l$层中神经元的数量（不包括偏置神经元）。  
在神经网络中使用的代价函数是在逻辑回归中使用的正则化代价函数：  
$$J(θ)=-\frac{1}{m}[∑_{i=1}^m y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))]+\frac{λ}{2m}∑_{j=1}^n θ_j^2$$
略微不同的是，在神经网络中分类标签和假设函数的输出值都变成了k维的向量，因此神经网络中的代价函数变成了：  
$$J(θ)=-\frac{1}{m}[∑_{i=1}^m ∑_{k=1}^Ky_k^{(i)} log⁡(h_θ(x^{(i)} )_k)+(1−y_k^{(i)}) log(1−h_θ (x^{(i)})_k)]+\frac{λ}{2m}∑_{l=1}^{L-1}∑_{j=1}^{s_l}∑_{j=1}^{s_l+1} (Θ_{ji}^{(l)})_j^2$$   
解释：  
1. 用$(h_Θ(x))_i$来表示第i个输出  
2. 这个代价函数中$∑_{k=1}^K$表示所有的输出单元之和，这里我们主要是将$y_k$的值与$(h_Θ(x))_k$的大小作比较   
3. 正则项的作用是去除那些对应于偏置单元的项，具体而言就是不对$i=0$的项进行求和和正则化。  