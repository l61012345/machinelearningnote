---
title: 6. 大作业-创建一个交通标志分类器
categories: 
- Machine Learning-NUS 2021
- 课后练习
---
# Traffic Sign Recognition
Traffic-sign recognition (TSR) is a technology by which a vehicle is able to recognize the traffic signs
put on the road e.g. "speed limit" or "children" or "turn ahead". This is a very important technology
in self-driving cars.    
This project will give you the chance to train different models using various features to classify traffic
signs.   
The project is divided into 3 difficulty levels. Beginner, Expert and Bonus.   
## Beginner Level   
For this level we use the Chinese Traffic Sign Database (Traffic Sign Recogntion Database (ia.ac.cn)).
This is available as a zip file in your project folder under the name “Dataset_1.zip”.
This dataset consists of 5998 images belonging to 58 classes. Each image is named “XXX_yyyy.png”.
Here XXX represent the class (traffic sign type) and yyyy represents the image number within each
class.
For the beginner level, we make use of the “starter.py” code, which you can find in the project
directory.
Follow along with the tasks and fill in the blanks of the given code to complete beginner level.
Follow the tasks with “starter.py” and fill in the missing code for each section.
### T1: Reading images.
- Change the dataset_path to point to the unzipped Dataset_1/images folder in your
computer.
- The given loop will go through all the files in the folder, variable i gives each file name.
- Complete the code to read the images and append them to list X
- The labels for each image has been already appended to list y for you
At the end of T1, you should have X, y with 5998 entries on each.
### T2: Pre-processing images.
- Given loop will go through all images in X and resize them to 48x48 pixels.
- Complete the code to convert the images to grayscale. (Hint: use the cvtColor function in
opencv)
- Complete the code to append the pre-processed images to X_processed list.
At the end of T2, you should have X_processed with 5998 entires of resized and grayscale images.
T3: Calculating Features and Splitting train/test sets.
- Install skimage using anaconda. (you can follow the same instructions given for installing
sklearn with the package name “scikit-image”)
- The given code will use skimage and extract hog features for you.
- Write code to split X_features and y into training and testing sets. Make use of the
“sklearn.model_selection.train_test_split” to do this. Use a 80-20 split and make sure to
shuffle the samples.
At the end of T3, you should have x_train, x_test, y_train and y_test. Training sets should have 4798
samples and the test sets should have 1200 samples.
T4: Training and testing the classifier.
- Use the sklearn SVM package to train a classifier using x_train and y_train.
- Use the x_test and y_test to evaluate the classifier and print the accuracy value.
## Expert Level:
We will build upon the beginner level code to try out different techniques and improve our model.
The same dataset will be used here.
Complete the following tasks,
### T1: Different pre-processing techniques
What are other pre-processing steps you can use?
Examples: Keep 3 channels (RGB), add a gaussian blur to reduce noise, etc.
Try few other pre-processing techniques and evaluate how they affect accuracy
### T2: Different features
What are other feature extraction methods you can use?
Explore some other feature extraction methods given in skimage (Module: feature —
skimage v0.19.0.dev0 docs (scikit-image.org))
Try few other feature extraction methods and evaluate how they affect accuracy, you can
also try different packages here (no need to stick with skimage)
### T3: Different Classification Models
What are other classification models you can use?
Try other classifiers including but not limited to; RandomForrest, kNN and Decision Tree.
For each classifier, change parameters and evaluate how the parameters affect accuracy.
## Bonus Level
This level is for you to apply what you learned to a more challenging dataset from scratch. The
dataset is, German Traffic Sign Recognition Benchmark (GTSRB) (German Traffic Sign Benchmarks
(rub.de)). This is available in the “Dataset_2.zip” file.
This dataset is already split into training and testing sets for you.
### Task
Write a python program to load the images from this dataset, into X, y. Then do suitable preprocessing, feature extraction and model training to develop a Traffic Sign Recognition system.
Report on the methods used and the results obtained by your Traffic Sign Recognition system.


```
import numpy as np
import glob
from cv2 import cv2
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from sklearn import metrics
from skimage.feature import hog
from skimage.feature import canny
from skimage.feature import local_binary_pattern

from sklearn import svm
import sklearn.naive_bayes as sk_bayes
from sklearn.neighbors import KNeighborsClassifier

import datetime

# load the dataset
dataset_path = "Dataset_1\\images\\"
X = []  # 图像集
y = []  # 标签集
print("Loading from the dataset...")
for i in glob.glob(dataset_path + '*.png', recursive=True):
    label = i.split("images")[1][1:4]  # 根据文件名开头划分标签
    y.append(label)
    img = cv2.imread(i)
    X.append(i)  # 加载图像

# Preprocess
x_prossessed = []
print("Preprocessing the images...")
for x in X:
    x = cv2.imread(x)
    temp_x = cv2.resize(x, (48, 48))
    # 图像处理部分，给出了如下三种图像处理的方式，可以组合，也可以单独使用，不需要的直接注释掉然后append（36行）对应的结果就行
    # convert to gray 转换为灰度图
    x_process_gray = cv2.cvtColor(temp_x, cv2.COLOR_BGR2GRAY)
    # gave the gaussian blur 高斯滤波
    x_process_gau = cv2.GaussianBlur(x_process_gray, (5, 5), 0)
    # gave the medianBlur 中值滤波
    x_process_md = cv2.medianBlur(x_process_gau, 3)
    x_prossessed.append(x_process_md)

# calculate features
# 提取特征，给出了下面的三种方法，三选一，选一个取消注释。
X_features = []
print("Calculating features...")
for x in x_prossessed:
    '''
    # Hog features : 提取图像的直方图信息
    x_feature_hog = hog(x,
                    orientations=8,
                    pixels_per_cell=(10, 10),
                    cells_per_block=(1, 1),
                    visualize=False,
                    multichannel=False)
    X_features.append(x_feature_hog)
    '''
    '''
    # canny features： 提取图像的边缘特征
    x_feature_canny = canny(np.array(x), sigma=1.0)
    X_features.append(x_feature_canny)
    '''
   
    # LBP features: 对光照有很强的鲁棒性
    x_feature_lbp = local_binary_pattern(x, 8, 1.0, method='default')
    X_features.append(x_feature_lbp)
    
# 把X_features 转换为长向量
images = np.array(X_features).reshape((len(np.array(X_features)), -1))

# 划分训练集和测试集
print("Trainning the dataset...")
x_train, x_test, y_train, y_test = train_test_split(images,
                                                    y,
                                                    test_size=0.2,
                                                    shuffle=True)

# 分类器，下面给出了三种分类器
acc = []  # 创建一个列表用于存放准确率
timecosts = []  # 创建一个列表用于存放耗时
# 支持向量机分类器
print("Trainning by svm...")
time1 = (datetime.datetime.now())  # 第一个时间戳
clf1 = svm.SVC()
clf1.fit(x_train, y_train)  # 数据拟合
time2 = (datetime.datetime.now())  # 第二个时间戳
acc_svm = clf1.score(x_test, y_test)  # 计算准确率
acc.append(acc_svm)
timecost = str(time2 - time1)  # 计算时间差
timecosts.append(timecost)

# 伯努利分布的朴素贝叶斯分类器
print("Trainning by BN...")
time1 = (datetime.datetime.now())  # 第一个时间戳
clf2 = sk_bayes.BernoulliNB(alpha=1.0,
                            binarize=0.0,
                            fit_prior=True,
                            class_prior=None)
clf2.fit(x_train, y_train)  # 数据拟合
time2 = (datetime.datetime.now())  # 第二个时间戳
acc_NB = clf2.score(x_test, y_test)  # 计算准确率
acc.append(acc_NB)
timecost = str(time2 - time1)  # 计算时间差
timecosts.append(timecost)

# KNN分类器 k=1 (对于这个KNN，k越大，越不行)
print("Trainning by KNN...")
time1 = (datetime.datetime.now())  # 第一个时间戳
clf3 = KNeighborsClassifier(n_neighbors=1)
clf3.fit(x_train, y_train)  # 数据拟合
time2 = (datetime.datetime.now())  # 第二个时间戳
acc_KNN = clf3.score(x_test, y_test)  # 计算准确率
acc.append(acc_KNN)
timecost = str(time2 - time1)  # 计算时间差
timecosts.append(timecost)

clfname = ['SVM', 'NB', 'KNN']
print("---------result-------------")
print("accurancy is:")
print(clfname)  # 输出分类器的名字
print(acc)  # 输出准确率
print("timecost:")
print(timecosts)

print("Displaying the Confusion Matrixes")
# 显示混淆矩阵
# 混淆矩阵显示有点慢 但是三张都可以显示 【要关闭当前之后才能显示下一张】
# svm算法的混淆矩阵
print("cm of svm")
cm1 = metrics.plot_confusion_matrix(clf1, x_test, y_test)  # 创建混淆矩阵
plt.get_current_fig_manager().window.state('zoomed')
plt.show()  # 显示混淆矩阵

# BN算法的混淆矩阵
print("cm of bn")
cm2 = metrics.plot_confusion_matrix(clf2, x_test, y_test)  # 创建混淆矩阵
plt.get_current_fig_manager().window.state('zoomed')
plt.show()  # 显示混淆矩阵

# KNN的混淆矩阵
print("cm of knn")
cm3 = metrics.plot_confusion_matrix(clf3, x_test, y_test)  # 创建混淆矩阵
plt.get_current_fig_manager().window.state('zoomed')
plt.show()  # 显示混淆矩阵

```