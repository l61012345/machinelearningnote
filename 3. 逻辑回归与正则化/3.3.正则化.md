# 正则化
## 过拟合问题 
对于模型，如果一个模型对于数据的偏差很大，不能能够很好的拟合数据的分布，称为欠拟合，或者说这个算法具有高偏差的特性。 如果一个模型虽然可以穿过所有的数据点，但是其图像波动很大，其同样也不能描述数据的分布，（其数据的分布是无法被泛化处理），称为过拟合，或者说这个算法具有高方差的特性。 在这种情况下，模型的参数过于多（有可能代价函数正好为0），以至于我们可能没有足够多的数据去约束他，来获得一个假设函数。
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224205854.png)
过拟合现象往往会发生在参数过多，而训练样本过少的情况。减小过拟合现象的思路有两种： 
1. 尽可能的去掉那些影响因素很小的变量，这种方法虽然解决了过拟合问题，但是损失了精度。  
2. 正则化  

## 代价函数的正则化  
对于代价函数：
$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^(i))-y^{(i)})^2$$
增加两个惩罚项$1000\theta^2_3$和$1000\theta^2_4$，代价函数变为：  
$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^(i))-y^{(i)})^2+1000\theta^2_3+1000\theta^2_4$$
如果要最小化这个函数，那么$\theta_3$与$\theta_4$就要尽可能的接近0，那么最后拟合的结果（假设函数）：$\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$，仍然是一个类似的二次函数.  
正则化的基本思想是**如果所有的参数足够小，那么假设模型就更简单。**  
>事实上，如果参数足够小，得到的函数就会越平滑，越简单，越不容易出现过拟合的问题  

在实际上，对于大量的特征和大量的参数，比如$x_1..x_{100}$和$\theta_0...\theta_{100}$，我们无法确定哪些参数是高阶项的参数，这个时候采用的方法就是对代价函数进行修改，使得所有的参数都尽可能的小。  
新的代价函数方程：  
$$ J_{\theta}=\frac{1}{2m}[\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+λ\Sigma_{j=1}^{m}\theta_j^2]$$
$λ\Sigma_{j=1}^{m}\theta_j^2$称为正则化项，它的目的是为了缩小每一项的参数
>$\theta_0$是否正则化对结果影响不大  
λ的作用是对+的前后（前：更好的拟合训练集，后：假设函数足够简单）两项进行取舍平衡，称为正则化系数  

如果λ被设置的太大，那么所有参数的惩罚力度被加大，这些参数最后的结构都将全部接近于0，那么最后的假设函数将会变成$h_\theta(x)=θ_0$,最终导致欠拟合。  

## 线性回归的正则化
**正则化的梯度下降算法**  
在线性回归中，我们使用修改后的梯度下降算法：  
Repeat {   
$$θ_0:=θ_0-\alpha\frac{1}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(j)} \tag{1}$$
> $θ_0$  不需要正则化  
$$θ_j:=θ_j-\alpha[\frac{1}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}θ_j] \tag{2}$$
$$j=1,2,3,...,n$$
}
> 事实上  $\frac{1}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(j)}+\frac{λ}{m}θ_j=\frac{∂J(θ)}{∂θ_j}$  
> $\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}=\frac{∂J(θ)}{∂θ_0}$  

如果将（2）中的$\theta_j$统一，那么就可以得到（3）：  
$$θ_j:=θ_j（1-α\frac{λ}{m}）-\frac{α}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)} \tag{3}$$

由于$1-α\frac{λ}{m}<1$,且只比1小一点点，也就是说，梯度下降算法每次更新的时候$θ_j$一开始都会比原来小一点点，再进行原来的梯度下降更新  
  

**正则化的正规方程算法**  
在之前的讲义中，探讨过设计两个矩阵：  
$X=\begin{bmatrix} (x^{(1)})^T \\ ...\\ (x^{(m)})^T \end{bmatrix}$ 代表有m个数据的数据集 和 $y=\begin{bmatrix} y^{(1)} \\ ...\\ y^{(m)} \end{bmatrix}$ 代表训练集当中的所有的标签  
通过
$$θ=(X^TX)^{-1}X^Ty$$
（相当于对$J(θ)$中的每一个θ求偏导数，并且使其等于0）  
可以求出最适合的θ  
现在改变在正规方程中加入一项：
$$θ=(X^TX+λ
\begin{bmatrix}
0 & 0 & 0 & ...&0 \\   0 & 1 & 0& ...&0 \\ 0 & 0 & 1& ...&0 \\... & ... & ...& ...&...\\0 & 0 & 0& ...&1
\end{bmatrix})^{-1}X^Ty$$
来达到同样的效果  
>$\begin{bmatrix}
0 & 0 & 0 & ...&0 \\   0 & 1 & 0& ...&0 \\ 0 & 0 & 1& ...&0 \\... & ... & ...& ...&...\\0 & 0 & 0& ...&1
\end{bmatrix}$是一个(n+1)的方阵  

如果矩阵X不可逆$（m<=n）$,那么$(X^TX)^{-1}$也同样不可逆,但是经过数学证明，无论如何$(X^TX+λ
\begin{bmatrix}
0 & 0 & 0 & ...&0 \\   0 & 1 & 0& ...&0 \\ 0 & 0 & 1& ...&0 \\... & ... & ...& ...&...\\0 & 0 & 0& ...&1
\end{bmatrix})^{-1}$ 都是可逆的。