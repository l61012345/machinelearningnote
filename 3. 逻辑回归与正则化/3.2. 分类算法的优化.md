# 分类算法的优化
## 其他代价函数优化算法
对于代价函数，它可以被拆分成求$J(θ)$和求$\frac{∂J(θ)}{∂θ_j }$  两个基础的部分  

事实上，除了基础的梯度下降算法能够求到最小值之外，还有如下的集中基本方法：
1. 共轭梯度算法（Conjugate Gradient）
2. BFGS
3. L-BFGS

这些算法有一些共同的特点：
1. 这些算法利用线搜索算法（一种智能内循环）不需要手动选择学习率 α。
2. 收敛的速度高于梯度下降算法
3. 复杂度高于梯度下降算法

## 多分类问题
对于同一个数据集x，需要分类的目标不只有两种，意味着离散取值y的值不只有0，1两个值。  
基本思想是**将一个n元分类问题转化为n个二分类问题**。  
比如对y=1，2，3:  
可以先将2，3 设置为负类，使用之前的逻辑分类方法就能够将1与（2，3）分开，重复三次，能得到三个逻辑斯蒂函数：$h_θ^i (x)i=1,2,3$。  
由于$h_θ^i (x)=P(y=i│x; θ)$， 因此$h_θ^i (x)$表示将1设置为正类别，分类器中是1的概率。   
最后给定函数$max(h_θ^i (x))$， 表示选择出三个当中概率最高的部分，作为判断的结果。

