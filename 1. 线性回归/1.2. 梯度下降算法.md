<style>
img{
    width: 40%;
    padding-left: 20%;
}
</style>
# 梯度下降算法
在开始之前，为了方便解释，首先规定几个符号所代表的意义：  
$m$ 训练集中训练样本的数量  
$X$  输入变量  
$Y$  输出变量  
$(x,y)$ 训练样本  
$(x^i,y^i)$第i个训练样本（i表示一个索引）  
## 监督学习算法的流程
提供训练集>学习算法得到$h$（假设函数：用于描绘x与y的关系）>预测y 的值  
## 代价/损失函数（Cost function）  
**假设函数(Hypothesis function)**——$h$是用来表示某一个数据集可能存在的线性/非线性关系的函数。对于线性拟合，其假设函数为：  
$$h_θ(x)=θ_1x+θ_0$$
这其中的$θ$是假设函数当中的参数。
也可以简化为：
$$h_θ(x)=θ_1x$$  
**代价函数**，在统计学上称为均方根误差函数。当假设函数中的系数$θ$取不同的值时，$\frac{1}{2m}$倍假设函数预测值$h_θ(x^{(i)})$和真实值$y^{(i)}$的差的平方的和之间的函数关系表示为代价函数$J$。
$$J(θ_0,θ_1)=\frac{1}{2m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2$$ 
> 在这里取1/2的原因是便于消除求导之后产生的2倍,同时也可以进一步缩小$θ$  

> 该函数的自变量是$θ_1$和$θ_0$，因此该函数是三维的函数（如图所示）。  
 ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131130651.png)  

**代价函数在几何上表示为数据集空间内的各点到假设函数的距离的平方的平均值的一半。**  
要想使得数据能够被假设函数很好地拟合，那么代价函数要尽量地小。**当代价函数取到它的最小值即$J(θ_1)_{min}$时，此时的填入假设函数的$θ$对数据的拟合程度是最好的。**  
对于线性的代价函数，假设函数对数据集的拟合程度越高，对应的$(θ_0,θ_1)$越接近代价函数图像等高线的中心。  
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131132852.png)   
## 梯度下降算法（Gradient Descent）  
### 梯度
在微积分中，函数$f(x,y)$在$(x_0,y_0)$处是函数值增加最快的方向是**梯度（Gradient）**的方向，**梯度的反方向是函数值减小最快的方向。**  
梯度的计算公式：  
$$▿f|_{(x_0,y_0)}=(f_x(x_0,y_0),f_y(x_0,y_0))$$
### 概述
梯度下降算法是一种求解代价函数最小值的方法，它可以用在**多维任意的假设函数**当中。  
简而言之，梯度下降算法求得$J(θ_1)_{min}$的主要思路是：   
1. 给定$θ_0$和$θ_1$的初始值，通常令$θ_0=0$，$θ_1=0$。
2. 不断改变$θ_0$和$θ_1$的值使得$J(θ_0,θ_1$的值逐渐变小，直到找到$J(θ_0,θ_1)$的最小值或者局部最小值。  
如果从一个初始值出发，寻找附近的最小值，重复该过程，得到上图，最后得到的值为局部最优解。  
> 将梯度下降算法类比为爬山，从一个点开始，不断寻找“下山”的路线，最后找到一个“下山”的出口。——吴恩达  

当改变初始值时，会找到另一条“下山”的路径，找到第二个局部最优解（局部最小值）。  
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131134419.png)  
对于线性回归的代价函数而言，只存在一个局部最小值。（见代价函数的图像）  
### 表示  
梯度下降算法可以表示为：  
Repeat untill convergence{  
$$θ_j:=θ_j-α\frac{∂J(θ_0,θ_1)}{∂θ_j},j=0~and~j=1$$
}   
解释：    
1. :=  表示赋值运算符
2. α称为**学习率**，用来控制下降的**步长**（Padding），即更新的幅度：  
         如果α太小，同步更新的速率会非常的慢   
         而α过大，同步更新时可能会越过最小值点   
3. $\frac{∂J(θ_0,θ_1)}{∂θ_j}$是代价函数的梯度：
$$\frac{∂J(θ_0,θ_1)}{∂θ_0}=\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$ 
$$\frac{∂J(θ_0,θ_1)}{∂θ_1}=\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$   
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131135144.png)  
△在代价函数中（以简化的代价函数为例），无论初始值在最小值点的左侧还是右侧，通过同步更新都能够使该点被“移动（Update）”到最小值，在最小值点，由于导数值为0，最终同步更新停止在了$θ_j=θ_j$，如前面所说，$θ_j$即为极小值点。  

### 同步更新  
**同步更新**（Simulaneous update）是实现梯度下降算法的最有效方式。  
$$temp0:~~θ_0:=θ_0-α\frac{∂J(θ_0,θ_1)}{∂θ_0}$$
$$temp1:~~θ_1:=θ_1-α\frac{∂J(θ_0,θ_1)}{∂θ_1}$$
$$θ_0:=temp0$$
$$θ_1:=temp1$$
这个更新方程能够同时更新$θ_0$和$θ_1$：$θ_0$更新为$θ_0-J'(θ)$，对$θ_1$同理。  
更新的方法是计算赋值号右边带入$θ_1$和$θ_2$的值进行计算，得到的两个值分别储存在temp0和temp1中，从上到下进行赋值。  

对于简化的代价函数：  
$$θ_1：=θ_1-αJ'(θ_1)$$  
$$\frac{dJ(θ_1,θ_0)}{dθ_j} =d(\frac{1}{2m}Σ(h_θ(x_i)-y_i))^2)$$   


将梯度代回代价函数中就得到了**Batch梯度下降法**的基本形式：  
Repeat untill convergence{  
$$θ_0:=θ_0-α\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$
$$θ_1:=θ_1-α\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$
}    
