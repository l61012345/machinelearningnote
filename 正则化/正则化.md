# 正则化
## 过拟合问题 
对于模型，如果一个模型对于数据的偏差很大，不能能够很好的拟合数据的分布，称为欠拟合，或者说这个算法具有高偏差的特性。 如果一个模型虽然可以穿过所有的数据点，但是其图像波动很大，其同样也不能描述数据的分布，（其数据的分布是无法被泛化处理），称为过拟合，或者说这个算法具有高方差的特性。 在这种情况下，模型的参数过于多（有可能代价函数正好为0），以至于我们可能没有足够多的数据去约束他，来获得一个假设函数。
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224205854.png)
过拟合现象往往会发生在参数过多，而训练样本过少的情况。减小过拟合现象的思路有两种： 
1. 尽可能的去掉那些影响因素很小的变量，这种方法虽然解决了过拟合问题，但是损失了精度。  
2. 正则化  

## 代价函数的正则化  
对于代价函数：
$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^(i))-y^{(i)})^2$$
增加两个惩罚项$1000\theta^2_3$和$1000\theta^2_4$，代价函数变为：  
$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^(i))-y^{(i)})^2+1000\theta^2_3+1000\theta^2_4$$
如果要最小化这个函数，那么$\theta_3$与$\theta_4$就要尽可能的接近0，那么最后拟合的结果（假设函数）：$\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$，仍然是一个类似的二次函数.  
正则化的基本思想是**如果所有的参数足够小，那么假设模型就更简单。**  
>事实上，如果参数足够小，得到的函数就会越平滑，越简单，越不容易出现过拟合的问题  

在实际上，对于大量的特征和大量的参数，比如$x_1..x_{100}$和$\theta_0...\theta_{100}$，我们无法确定哪些参数是高阶项的参数，这个时候采用的方法就是对代价函数进行修改，使得所有的参数都尽可能的小。  
新的代价函数方程：  
$$ J_{\theta}=\frac{1}{2m}[\Sigma_{i=1}^{m}(h_θ(x^(i))-y^{(i)})^2+λ\Sigma_{j=1}^{m}\theta_j^2]$$
$λ\Sigma_{j=1}^{m}\theta_j^2$称为正则化项，它的目的是为了缩小每一项的参数
>$\theta_0$是否正则化对结果影响不大  
λ的作用是对+的前后（前：更好的拟合训练集，后：假设函数足够简单）两项进行取舍平衡，称为正则化系数  

如果λ被设置的太大，那么所有参数的惩罚力度被加大，这些参数最后的结构都将全部接近于0，那么最后的假设函数将会变成$h_\theta(x)=θ_0$,最终导致欠拟合。  


