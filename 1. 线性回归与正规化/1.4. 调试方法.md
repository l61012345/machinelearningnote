# 调试方法
## 特征缩放
对于某些不具有比较性的样本特征$x_i$ （比如对其他的x来说$x_i$ 相当大或者相当小），梯度下降的过程可能会非常漫长，并且可能来回波动才能最后收敛到全局的最小值。   
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144432.png)    
在这样的情况下，可以对$x_i$ 进行缩放（如 $x_i≔αx_i$  或者 $x_i=x_i/α$），使得$x_i$ 与其他的$x$具有可比性，以增加梯度下降的效率。  
**通常将$x$缩放至⟦-1,1⟧**的区间内。（只表示一个大致的范围，这不是绝对的。）

## 均值归一
将$x_i$  替换为$x_i−μ_i$ 使得特征值具有为0的平均值（对$x_0$ 不适用）
$$x_i:=(x_i−μ_i)/s_i$$ 
定义$μ_i$  为训练集$X$ 的平均值，$s_i=|x_imax−x_imin |$, 表示$x_i$ 的取值范围（近似值），或者直接设置为$s_i$ 的标准差。

## 学习率
梯度下降调试的方法：
1. 绘制minJ（θ）-batch的图像  
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144740.png)
原则：每一个batch之后 θ 的值都应该减小，这样的图像能够通过直观地表现变化率来表现梯度下降是否收敛（变化率为0）。  

2. 自动收敛测试  
如果$J(θ)$在某一次迭代之后的下降值小于某个值$ε$后，就能够判断算法已经达到了收敛。  
$ε$的值比较难取，所以通常采取1.中的方法进行观测。

常见的α过大的minJ（θ）-batch的图像：
α过大,导致代价函数无法收敛  
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144816.png)   
  		
α过小，导致代价函数收敛速度过慢
