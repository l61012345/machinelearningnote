<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>
# 神经网络的基本模型
## 神经元模型 
- 假设： 大脑对于不同功能（听觉，视觉，触觉的处理）的实现是依赖于同样的学习方法  
- 依据： 神经重接实验  
- 神经元模型  
  神经网络模拟了大脑中的神经元或者是神经网络。先来看大脑中的神经元构成，我们会发现神经元有很多的输入通道（树突），同时通过轴突给其他的神经元传递信号。  将神经元简单抽象：一个计算单元，它从输入端接收一定数目的信息，并作一些处理，并将结果传递给其他的神经元。
      
在计算机中，我们构建一个逻辑单元，它从输入端接收数据集X，并作处理来生成一个激活函数$h_θ (x)=\frac{1}{1+e^{-θ^T X}}$
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20201229183932.png)  
在这个模型之上，输入端会额外增加一个$x_0=1$，称为偏置单元。  
在神经网络中，$Θ$称为模型的权重，$g(z)=\frac{1}{1+e^{-z}}$称为激活函数。  

神经网络是一组神经元连接在一起的集合，如图所示  
![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20201229184740.png)  
第一层称为输入层，我们在这一层输入全部的特征，最后一层称为输出层，这一层的神经元输出假设的最终结果，中间的层称为隐藏层，隐藏层可能不止有一层。  
统一地，$a_i^{(j)}$将表示第j层的第i个激活项（激活指计算并输出结果），同时，第j层到第j+1层之间的映射由参数矩阵$Θ^{(j)}$确定，那么上图就可以用公式表示为：
$$a_1^{(2)}=g(Θ_{10}^{(1)}x_0+Θ_{11}^{(1)}x_1+Θ_{12}^{(1)}x_2+Θ_{13}^{(1)}x_3$$
$$a_2^{(2)}=g(Θ_{20}^{(1)}x_0+Θ_{21}^{(1)}x_1+Θ_{22}^{(1)}x_2+Θ_{23}^{(1)}x_3$$
$$a_3^{(2)}=g(Θ_{30}^{(1)}x_0+Θ_{31}^{(1)}x_1+Θ_{32}^{(1)}x_2+Θ_{33}^{(1)}x_3$$
$$h_{Θ}(x)=g(Θ_{10}^{(2)}a_0^{(2)}+Θ_{11}^{(2)}a_1^{(2)}+Θ_{12}^{(2)}a_2^{(2)}+Θ_{13}^{(2)}a_3^{(2)})$$
>如果一个网络在第j层有$s_j$个单元，且在第j+1层有$s_j+1$个单元，那么矩阵$Θ^{(j)}$的维度为$s_{j+1} \times (s_j+1)$

## 神经网络的向量化:前向传输
对如上的等式，现在将$g()$中的线性加权组合以$z^{(2)}_1,z^{(2)}_2,z^{(2)}_3$表示，那么就有：
$$a_1^{(2)}=g(z_1^{(2)})$$
$$a_2^{(2)}=g(z_2^{(2)})$$
$$a_3^{(2)}=g(z_3^{(2)})$$
现在就能够定义三个向量使得上述等式转化为向量乘法：  
$x=\begin{bmatrix}x_0 \\x_1\\x_2\\x_3 \end{bmatrix}$,
$z^{(2)}=\begin{bmatrix} z_1^{(2)}\\z_2^{(2)}\\z_3^{(2)}\end{bmatrix}=Θ^{(1)}x$，$a^{(2)}=\begin{bmatrix} a_1^{(2)}\\a_2^{(2)}\\a_3^{(2)}\end{bmatrix}$  
那么上述等式最终就可以转化成：
$$z^{(2)}=Θ^{(1)}x$$
$$a^{(2)}=g(z^{(2)})$$
对于隐藏层的偏置单元，增加一项$a_0^{(2)}=1$.
最后计算$z^{(3)}=Θ^{(2)}a^{(2)}$,那么最终得到的假设模型将会是：
$$h_{Θ}(x)=a^{(3)}=g(z^{(3)})$$

单看layer2 和 layer3，我们发现这一层其实做的就是逻辑回归，但输入进逻辑回归的特征不再是原始的特征x，而是通过原始特征生成的特征a.
而a与x之间的关系通过θ来定义。 因此可以通过改变θ来改变输入层和隐藏层之间的关系。